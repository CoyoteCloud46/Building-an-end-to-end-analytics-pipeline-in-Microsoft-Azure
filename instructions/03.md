## Exercise 3: Create Schemas, Tables, Stored Procedures and Datasets for SQL Pool.

Duration: 20 minutes

### Task 1: Create Schemas in SQL Pool.

1. Now you will create **Staging** schema by doing the following steps

    - Navigate to **Data** from the synapse studio -> Databases ->**SQL Pool**->**Schemas**->**New SQL Script**->**New Schema**
    - **Run** the command **CREATE SCHEMA [Staging]**.
  
    ![schemas ](images/17.png)
  
2. Now you will create **Backup** Schema by doing the following steps

    -  Navigate to **Data** -> Databases ->**SQL Pool**->**Schemas**->**New SQL Script**->**New Schema**
    - **Run** the command **CREATE SCHEMA [Backup]**.
    - **Publish** the changes made after creating both the schemas.
   
   
### Task 2: Create Tables in SQL Pool.
 
 1. Navigate to **Develop->SQL scripts**, Select and execute the SQL script **EXE3 CreateStagingBackupTables SETUP ONLY** that you imported in the previous exercise.Ensure you are connected to **sqlpool**
 
 2. Click on **Run**
 
 3. Go to **Data-> Databases->Sql pool->tables** to verify whether the tables are created .Refresh if you don't see the tables
 
    ![tables ](images/18.png)

 
### Task 3: Create Stored Procedures in SQL Pool.
 
1. Navigate to **Develop->SQL scripts**, Select and execute the SQL script **EXE3 CreateStoredProcedures SETUP ONLY** that you imported in the previous exercise. Make sure you are connected to **sqlpool**

2. Click on **Run** 

3. Go to **Data-> Databases->Sql pool->Progammability->Stored Procedures** to verify whether the stored procedures are created .Refresh if you don't see the stored procedures

   ![stored procs ](images/19.png)

### Task 4: Create Datasets for the pipeline.

1. Navigate to Data->**+**->**Dataset**

   ![datasets](images/020.png)

2. Now you will create **FHV Dataset**.

   - Search for **ADLS Data Lake Storage Gen 2** and Click on **Continue**.

   ![dataset gen 2](images/021.png)

   - Select **Parquet** as format of data and click continue.

   ![datasets](images/22.png)
 
   - Under Set Properties provide the name as **FHV**
   
   - Select the previously created Linked Service **CoreDataLakeStorageBackup**
   
   - Browse to the File Path nyctlc/fhv
   
   - Click on **OK**,the first dataset is now created

    ![fhv dataset](images/23.png)

3. Now you will create another dataset **FHVtoSQLBackup**.

   - Search for **Azure Synapse analytics** and Click on **Continue**.

    ![synapse](images/dataset2create.png)
 
   - Under Set Properties provide the name as **FHVtoSQLBackup**
   
   - Select the previously created Linked Service **CoreSQLPoolBackup**
   
   - Select table name as Backup.FHV
   
   - Click on **OK**,the dataset is now created
   
   - Click on **Publish All** after creating both the datasets.

   ![fhv dataset](images/dataset21.png)
   
4. To create remaining 15 datasets open the **Command Prompt** in the labvm
   
   ![command prompt](images/001.png)
   
**Note**: UploadSynapseFiles.ps1 is downloaded in C:\ in the labvm 

5. Execute the command by pressing enter **powershell -ExecutionPolicy Bypass C:\UploadSynapseFiles.ps1**
   
   ![command](images/002.png)

6. Minimise the internet explorer
   
   ![ie](images/003.png)

7. Output should be as shown below:
       
   ![output](images/004.png)

8. Navigate to **Data-> Linked->Datasets** to ensure 17 datasets are created. Refresh if you don't see the datasets.
   
   ![datasetupdate](images/datasetupdate.jpg)
   
   Click **Next** to go to the next exercise.
